---
title: "p8105_hw3_js5431"
author: "J Shearston"
date: "October 6, 2018"
output: 
  github_document:
    toc: true
---

```{r setup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))

```


## Problem #1

#### Data Import and Cleaning

```{r load & clean BRFSS}

library(p8105.datasets)

brfss = p8105.datasets::brfss_smart2010 %>% 
  janitor::clean_names() %>%
  rename(state = locationabbr, 
         county = locationdesc) %>% 
  filter(topic == "Overall Health") %>%
  arrange(year, state, county) %>% 
  mutate(response = ordered(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))) %>%
  select(year, state, county, response, data_value)


```


#### Questions and Responses

* In 2002, which states were observed at 7 locations?
    + In 2002, 3 states were observed at 7 locations: Connecticut, Florida, and North Carolina.
    
```{r states observed at 7 loc.}

 brfss %>% 
  filter(year == "2002") %>% 
  group_by(state) %>% 
  summarize(n_counties = n_distinct(county)) %>% 
  filter(n_counties == 7)

```
    

* Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.
    + This plot is horrifying. I removed the legend, as trying to determine which line represents which individual state is anxiety inducing. Overall, I would say that this plot shows that for most states, the total number of locations has remained fairly constant over time, with a few states having very large variability.

```{r spaghetti plot - obs by state}

brfss %>%
  group_by(year, state) %>% 
  summarize(n_counties = n_distinct(county)) %>% 
  ggplot(aes(x = year, y = n_counties, color = state)) +
  geom_line() +
  labs(
    title = "Total Observations",
    x = "Year",
    y = "No. of Observed Counties",
    caption = "Data from the BRFSS, p8105 datasets package") +
  theme(legend.position="none")
  
```


* Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.
    + The mean and standard deviation of the proportion of "Excellent" responses is pretty similar for NY state for all three years. 

```{r table}

brfss %>% 
  filter(state == "NY", 
         response == "Excellent", 
         year == 2002 | year == 2006 | year ==2010) %>% 
  group_by(year) %>% 
  summarize(excel_mean = mean(data_value),
            excel_sd = sd(data_value)) %>% 
  knitr::kable(digits = 2)

```


* For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.
    + Over time, the proportion of overall responses in each category seems to stay fairly consistent, such that "Very good" is consistently the most selected response, and "Poor" is the least selected. Within each category, there is some variability by state in the actual proportion of respondents who select each choice; interestingly, this variability sees similar for each choice, with the exception of "Poor", which seems to have a smaller variability.

```{r 5 panel plot}

brfss %>%
  group_by(year, state, response) %>% 
  summarise(avgby_response = mean(data_value, na.rm = (TRUE), round(2))) %>%
  ggplot(aes(x = year, y = avgby_response, color = state)) +
  geom_point() +
  facet_grid(~response) +
  theme(legend.position = "none")

```


## Problem #2

#### Data Import and Cleaning

```{r load and clean Instacart}

library(p8105.datasets)

icart = p8105.datasets::instacart %>% 
  janitor::clean_names()

```

#### Instacart Dataset Description

The Instacart dataset includes data from selected orders from an online grocery store; it contains `r nrow(icart)` rows and `r ncol(icart)` columns, and is formatted such that each row is a product from an order. Only one order for any particular individual is inlcuded.

Key variables include several unique IDs, such as order ID, product ID, and user ID; product information such as product name, aisle, and department; and interesting order information such as the time of day the order was placed and if the item has been ordered before. For example, an individual (user id = 112108) purchased 8 items as part of their order, including yogurt, milk, celery, cucumber, sardines, bananas, avacado, and string cheese. These items were ordered at 10 am, and 4 of them had been ordered by this customer in the past. 

#### Questions and Responses

* How many aisles are there, and which aisles are the most items ordered from?
    + There are 134 aisles, and the following aisles are most ordered from: Fresh vegetables, fresh fruits, packaged vegetables fruits, yogurt, and packaged cheese. 
    
```{r aisle info}

icart %>%
  summarize(dist_aisle = n_distinct(aisle_id, na.rm = TRUE))

icart %>% 
  group_by(aisle) %>% 
  summarize(n()) %>% 
  top_n(5)


```


* Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.
    + The produce department, which only appears to have 5 aisles, has by far the largest number of items ordered from it. Interestingly, the personal care department has a large number of aisles, but a much smaller amount of products are ordered from them. 
    
```{r items ordered plot}

icart %>% 
  group_by(department, aisle_id) %>% 
  summarize(items_aisle = n()) %>% 
  ggplot(aes(x = aisle_id, y = department)) +
  geom_point(aes(size = items_aisle), alpha = .5) +
  labs(
    title = "Number of Items Ordered, by Aisle",
    x = "Aisle ID",
    y = "Department",
    caption = "Data from Instacart, p8105 package"
  ) +
  scale_size(name = "Number of Items Ordered")
    
```


* Make a table showing the most popular item in the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
    + The most popular item in the packaged vegetables fruits aisle, organic baby spinach, was ordered 9,784 times, while the ost popular item in the dog food care aisle, a type of dog treat, was only ordered 30 times.

```{r pop. items in select aisles}

icart %>% 
  filter(aisle == "baking ingredients" | 
           aisle == "dog food care" |
           aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarise(prod_n = n()) %>% 
  top_n(1) %>% 
  knitr::kable()

```


* Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
    + On day 0 (Sunday) and day 5 (Friday), both Pink Lady Apples and Coffee Ice Cream are ordered at about the same time of day. However, on all other days of the week, Coffee Ice Cream is ordered later in the day than Pink Lady Apples. In other words, as the day goes on, perhaps icart shoppers' coffee ice cream cravings increase.  
    
```{r mean hour prod. ordered}

icart %>%
  filter(product_name == "Pink Lady Apples" |
           product_name == "Coffee Ice Cream") %>% 
  group_by(order_dow, product_name) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
  knitr::kable()

```


## Problem #3

#### Data Import and Cleaning

```{r noaa data import and viewing}

noaa = p8105.datasets::ny_noaa %>% 
  janitor::clean_names()

skimr::skim(noaa)

```

#### NOAA Dataset Description

The NOAA dataset contains select weather information for New York state, spanning the years 1981 to 2010, and contains `r nrow(noaa)` rows and `r ncol(noaa)` columns. Data is entered as location-date observations, with the following variables: location id, date, precipitation (tenths of mm), snow (mm), snow depth (mm), and max and min temperature (tenths of degrees C). There is a lot of missing data, and the dataset documentation mentions that not all locations collected information on snowfall amounts. 

#### Questions and Responses

* Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?
    + For snowfall, 0 cm is the most commonly observed value. This makes sense for a couple of reasons: in the state of New York, it does not smow most of the time, and when it does snow, it does not snow at all locations. Also, the data is currently recorded to one decimal point, and it is unlikely that many snowfalls are similar to the tenth of a cm. However, even after rounding to whole centimeters, 0 is still the most common response.

```{r clean noaa}

noaa = p8105.datasets::ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(tmax = as.numeric(tmax),
         tmin = as.numeric(tmin),
         tmax_c = tmax/10,
         tmin_c = tmin/10,
         prcp_mm = prcp/10,
         snow_cm = snow/10,
         snwd_cm = snwd/10,
         year = as.numeric(year)) %>% 
  select(id, year, month, day, tmax_c, tmin_c, prcp_mm, snow_cm, snwd_cm)

noaa %>% 
  ggplot(aes(x = snow_cm)) +
  geom_histogram()
  
noaa %>% 
  mutate(snow_cmr = round(snow_cm)) %>% 
  ggplot(aes(x = snow_cmr)) +
  geom_histogram()


```

    
* Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?
    + The max temperature in January appears to range between -10 and 10 degrees for all locations, and for July ranges between 20 and 32 degrees for all locations. There are a few outliers where max temperatures are either colder or warmer for speicfic locations during a specific year, but in general, there is not a lot of variability in max temperature by location for the state of New York, as temperatures tend to stay within a 10 degree band.

```{r two panel plot avg max temp}

noaa %>% 
  filter(month == "01" | month == "07") %>% 
  group_by(id, year, month) %>%
  filter(!is.na(tmax_c)) %>% 
  summarise(avg_max_temp = mean(tmax_c)) %>% 
  mutate(month = recode(month, "01" = "January", "07" = "July")) %>% 
  ggplot(aes(x = year, y = avg_max_temp, color = id)) +
  geom_line() +
  facet_grid(~month) +
  theme_bw() +
  theme(legend.position="none") +
  labs(
    title = "Avg. Max Temp by Location, NY",
    x = "Year",
    y = "Avg. Max Temp (C)",
    caption = "Data from NOAA, p8105 package"
  )

```


* Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
    + Answer

```{r plots of tmax v tmin and snowfall}

#library(patchwork)

temp_plot = ggplot(noaa, aes(x = tmax_c, y = tmin_c)) +
  geom_hex() +
  geom_smooth() +
  labs(
    title = "Max Temp vs Min Temp",
    x = "Max Temp (C)",
    y = "Min Temp (C)") +
  theme(legend.position = "right")

snow_plot = noaa %>%
  filter(snow_cm > 0 & snow_cm < 100) %>%
  mutate(year = as.factor(year)) %>% 
  ggplot(aes(x = year, y = snow_cm, fill = year)) +
  geom_boxplot() +
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(
    title = "Snowfall Density",
    x = "Year",
    y = "Snowfall (cm)")

#patchwork::temp_plot / snow_plot

```

